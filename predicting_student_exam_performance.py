# -*- coding: utf-8 -*-
"""Predicting_Student_Exam_Performance.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12L8c7juZRW84aEPGLnE2Z7-H8g08JYkK
"""

import pandas as pd
import numpy as np

data = pd.read_csv("/content/sample_data/mission1_data.csv")

data.head(10)

data['exam_scores'] = data['Math Score'] + data['Reading Score'] + data['Writing Score']
data= data.drop(['Math Score', 'Reading Score', 'Writing Score'], axis=1)
data['Test Preparation Course'].value_counts()

data.info()

data['Test Preparation Course'].fillna("Not Completed", inplace=True)

data.info()

data.head()

from sklearn.preprocessing import LabelEncoder, MinMaxScaler

import joblib

le = {}
for col in data.columns:
  if data[col].dtype == 'object':
    le[col] = LabelEncoder()
    data[col] = le[col].fit_transform(data[col])

# data['Gender'] = le.fit_transform(data['Gender'])
# data['Lunch Type'] = le.fit_transform(data['Lunch Type'])
# data['Test Preparation Course'] = le.fit_transform(data['Test Preparation Course'])
# data['Parental Education Level'] = le.fit_transform(data['Parental Education Level'])
joblib.dump(le, 'En.pkl')
le

# l = ['Gender', 'Lunch Type','Test Preparation Course', 'Parental Education Level' ]
# for i in l:
#   data[i] = le.fit_transform(data[i])

data.head(10)

data['Parental Education Level'].value_counts()

data.corr()

data.head()

data.describe()

import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(10, 8))
sns.histplot(data['exam_scores'], kde=False, bins=20, color='blue')
plt.title('Distribution of Exam Scores')
plt.xlabel('Math Score')
plt.ylabel('Frequency')
plt.show()

plt.figure(figsize=(10, 8))
sns.scatterplot(x='Study Time', y='exam_scores', data=data, color='blue')
plt.title('Relationship between Study Time and Exam Scores')
plt.xlabel('Study Time')
plt.ylabel('Exam Score')
plt.show()

plt.figure(figsize=(10, 8))
sns.boxplot(x='Parental Education Level', y='exam_scores', data=data, color='blue')
plt.title('Relationship between Parental Education Level and Exam Scores')
plt.xlabel('Parental Education Level')
plt.ylabel('Exam Score')
plt.show()

plt.figure(figsize=(10, 8))
sns.barplot(x='Parental Education Level', y='exam_scores', data=data, color='blue')
plt.title('Relationship between Parental Education Level and Exam Scores')
plt.xlabel('Parental Education Level')
plt.ylabel('Exam Score')
plt.show()

from sklearn.model_selection import train_test_split

x = data.drop('exam_scores', axis=1)
y = data['exam_scores']

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)
scaler = MinMaxScaler()
scaling_cols = ["Study Time", "Absences"]
x_train[scaling_cols] = scaler.fit_transform(x_train[scaling_cols])

# Also scale the test data using the same scaler
x_test[scaling_cols] = scaler.fit_transform(x_test[scaling_cols])



from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor

lr = LinearRegression()
lr.fit(x_train, y_train)
dt = DecisionTreeRegressor()
dt.fit(x_train, y_train)
rf = RandomForestRegressor()
rf.fit(x_train, y_train)

joblib.dump(scaler, 's.pkl')
joblib.dump(rf, 'rf_model.pkl')

from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error

y_pred_lr = lr.predict(x_test)
y_pred_dt = dt.predict(x_test)
y_pred_rf = rf.predict(x_test)

print("Linear regssion MSE: ", mean_squared_error(y_test, y_pred_lr))
print("Descion Tree MSE: ", mean_squared_error(y_test, y_pred_dt))
print("Random Forest MSE: ", mean_squared_error(y_test, y_pred_rf))

print("Linear Regression Accuracy: ", r2_score(y_test, y_pred_lr))
print("Decision Tree Accuracy: ", r2_score(y_test, y_pred_dt))
print("Random Forest Accuracy: ", r2_score(y_test, y_pred_rf))

print("Mean Absolute Error Linear regression:", mean_absolute_error(y_test, y_pred_lr))
print("Mean Absolute Error Descion tree:", mean_absolute_error(y_test, y_pred_dt))
print("Mean Absolute Error random forest:", mean_absolute_error(y_test, y_pred_rf))

plt.figure(figsize=(10, 8))
plt.scatter(y_test, y_pred_lr, color='blue', label='Linear Regression')
plt.scatter(y_test, y_pred_dt, color='green', label='Decision Tree')
plt.scatter(y_test, y_pred_rf, color='red', label='Random Forest')
plt.xlabel('Actual Exam Scores')
plt.ylabel('Predicted Exam Scores')
plt.title('Actual vs. Predicted Exam Scores')
plt.legend()
plt.show()

from sklearn.model_selection import GridSearchCV, RandomizedSearchCV

# Liner regression has no hyper pramters
# using gridsearchcv
dt_param = {'max_depth': [3, 5, 10, None], 'max_leaf_nodes': [2,5,7]}
grid_model = GridSearchCV(DecisionTreeRegressor(), dt_param, cv=5)
grid_model.fit(x_train, y_train)

print("best paramters is: ", grid_model.best_params_)

y_predict_grid = grid_model.best_estimator_.predict(x_test)
print("Mean Absoulte error for Grisearch is:", mean_absolute_error(y_test,y_predict_grid ))

r2_score(y_test, y_predict_grid)

rf_params = {'n_estimators': [50, 100, 200], 'max_depth': [3, 5,10]}
random_model = RandomizedSearchCV(RandomForestRegressor(),rf_params, scoring = 'neg_mean_absolute_error', random_state=42)
random_model.fit(x_train, y_train)

print('Randomized bes paramters is:', random_model.best_params_)

y_predict_rf = random_model.best_estimator_.predict(x_test)
print("Mean Absoulte error for Randomized is:", mean_absolute_error(y_test,y_predict_rf ))

pip show scikit-learn

